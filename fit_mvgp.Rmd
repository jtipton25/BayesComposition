---
title: "Test DM-Basis"
author: "John Tipton"
date: "6/8/2017"
output: html_document
---

# Simulation Study

To evaluate the performance of the proposed inverse prediction framework, we conducted tw different simulation studies. First, we simulated data on the log-scale random effect $\boldsymbol{\alpha}_i$ (Equation (3) in the manuscript) setting $N=800$ observations where the covariate is observed, $\tilde{N}=200$ observations where the covariate is unobserved with $d=8$ species. The parameters used in simulation are $\mu \sim \operatorname{N}(0, 0.125)$, $\mathbf{R} \sim \operatorname{LKJ}(1)$, $\tau_j \sim \operatorname{gamma}(5, 5)$, $X \sim \operatorname{N}(0, 1)$, $\sigma^2 = 1$, $\phi = 5$, and $\boldsymbol{\varepsilon}_i \sim \operatorname{N} ( \mathbf{0}, \boldsymbol{\Sigma}_{\varepsilon})$ with $ \boldsymbol{\Sigma}_{\varepsilon} \sim \operatorname{diag}(\boldsymbol{\tau}_{\varepsilon})^{1/2} \boldsymbol{R}_{\varepsilon}\boldsymbol{R}_{\varepsilon}' \operatorname{diag}(\boldsymbol{\tau}_{\varepsilon})^{1/2}$ where $\tau_{\varepsilon j} \sim \operatorname{gamma}(5, 5)$ $\boldsymbol{R}_{\varepsilon} \sim \operatorname{LKJ}(1)$. The simulation of the data using the `sim_compositional_data()` function in the `BayesComposition` package is shown below.


```{r, warning=FALSE, message=FALSE}
set.seed(11)
library(BayesComposition)
library(knitr)
library(ggplot2)
library(here)
```



```{r}
## Function from BayesComposition to simulate data from MVGP model
N <- 500
N_pred <- 200
d <- 8
n_knots <- 30

dat <- sim_compositional_data(
  N                          = N,
  N_pred                     = N_pred, 
  d                          = d,
  likelihood                 = "gaussian",        
  function_type              = "gaussian-process",
  predictive_process         = FALSE, 
  correlation_function       = "exponential",
  additive_correlation       = TRUE,
  multiplicative_correlation = TRUE)



simPlotData <- data.frame(
  species = as.factor(rep(1:d, each = N)),
  count   = c(dat$y),
  depth   = rep(dat$X, times = d),
  alpha   = c(dat$alpha))

gsim1 <- ggplot(simPlotData, aes(x=depth, y=count, color=species,
                                 group=species)) +
  geom_point(alpha=0.25) +
  theme(legend.position="none") +
  ggtitle("Simulated response vs. depth") +
  geom_line(aes(x=depth, y=alpha, col = species), simPlotData, lwd=1.25)

gsim2 <- ggplot(simPlotData, aes(x=depth, y=count, color=species,
                                 group=species)) +
  geom_point(alpha=0.25) + 
  theme(legend.position="none") +
  ggtitle("Simulated response vs. depth by species") +
  geom_line(aes(x=depth, y=alpha, col = species), simPlotData, lwd=1.25) +
  facet_wrap( ~ species, ncol = 4)

## generate the plot
multiplot(gsim1, gsim2, cols=1)
```

To fit the model, we consider $n^{\star} = 30$ evenly spaced knots over a range larger than the observed data $X$ to allow for potential predictions outside the range of the prediction. We have not found the inference to show much sensitivity in practical applications to the number of knots. We fit the model for 50000 MCMC iterations, where the first 25000 iterations include adaptive tuning of the Metropolis-Hastings proposals \citep{roberts}

```{r}
## generate a sequence of knots that covers ranges outside the observed values
X_knots <- seq(
  min(dat$X, na.rm=TRUE) - 1.25 * sd(dat$X, na.rm=TRUE), 
  max(dat$X, na.rm=TRUE) + 1.25 * sd(dat$X, na.rm=TRUE),
  length = n_knots)

## define the model parameters
params <- list(
  n_adapt              = 25000,
  n_mcmc               = 25000,
  n_thin               = 250, 
  correlation_function = "exponential", 
  likelihood           = "gaussian", 
  function_type        = "gaussian-process",
  n_chains             = 4,
  n_cores              = 4,
  n_knots              = n_knots,
  X_knots              = X_knots)

save_directory <- "./mvgp-test/"
progress_directory <- "./mvgp-test/"
save_file <- "mvgp.RData"
progress_file <- "mvgp-gaussian.txt"

if (file.exists(paste0(save_directory, save_file))) {
  load(paste0(save_directory, save_file))
} else {
  ## potentially long running MCMC code
  out <- fit_compositional_data(
    y                  = dat$y,
    X                  = dat$X,
    params             = params, 
    progress_directory = progress_directory,
    progress_file      = progress_file,
    save_directory     = save_directory,
    save_file          = save_file)
}
```


```{r}
## extract posterior samples
samples <- extract_compositional_samples(out)
mu_post <- samples$mu
eta_star_post <- samples$eta_star
zeta_post <- samples$zeta
Omega_post <- samples$Omega
phi_post <- samples$phi
sigma2_post <- samples$sigma2
tau2_post <- samples$tau2
R_post <- samples$R
R_tau_post <- samples$R_tau
xi_post <- samples$xi
```





```{r}
## Calculate Gelman-Rubin convergence statistic
Rhat <- make_gelman_rubin(out)
layout(matrix(1:9, 3, 3))
hist(Rhat, main="All parameters")
hist(Rhat[grepl("mu", names(Rhat))], main = "Rhat for mu")
hist(Rhat[grepl("eta_star", names(Rhat))], main = "Rhat for eta_star")
hist(Rhat[grepl("zeta", names(Rhat))], main = "Rhat for zeta")
hist(Rhat[grepl("phi", names(Rhat))], main = "Rhat for phi")
hist(Rhat[grepl("sigma2", names(Rhat))], main = "Rhat for sigma2")
hist(Rhat[grepl("tau2", names(Rhat))], main = "Rhat for tau2")
hist(Rhat[grepl("R_tau", names(Rhat))], main = "Rhat for R_tau")
hist(Rhat[grepl("xi", names(Rhat))], main = "Rhat for xi")
Rhat[!is.finite(Rhat)] <- NA
max(unlist(na.omit(Rhat)))
```

<!-- ```{r, eval=TRUE} -->
<!-- ##  -->
<!-- ## Posterior plots -->
<!-- ## -->

<!-- layout(matrix(1:4, 2, 2)) -->
<!-- matplot(beta_post[, , 1], type='l') -->
<!-- abline(h=dat$beta[, 1], col='red', lwd=2) -->
<!-- matplot(beta_post[, , 2], type='l') -->
<!-- abline(h=dat$beta[, 2], col='red', lwd=2) -->
<!-- matplot(beta_post[, , 3], type='l') -->
<!-- abline(h=dat$beta[, 3], col='red', lwd=2) -->
<!-- matplot(beta_post[, , 4], type='l') -->
<!-- abline(h=dat$beta[, 4], col='red', lwd=2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- layout(matrix(1:4, 2, 2)) -->
<!-- matplot(alpha_post[, 1, ], type='l') -->
<!-- abline(h=dat$alpha[1, ]) -->
<!-- matplot(alpha_post[, 2, ], type='l') -->
<!-- abline(h=dat$alpha[2, ]) -->
<!-- matplot(alpha_post[, 3, ], type='l') -->
<!-- abline(h=dat$alpha[3, ]) -->
<!-- matplot(alpha_post[, 4, ], type='l') -->
<!-- abline(h=dat$alpha[4, ]) -->
<!-- ``` -->

```{r}
alpha_post_mean <- t(apply(mu_post, 2, mean) + 
                       t(apply(zeta_post, c(2, 3), mean)))

fitPlotData <- data.frame(species=as.factor(rep(1:d, each=N)), 
                          count=c(dat$y), 
                          depth=rep(dat$X, times=d), 
                          alpha=c(alpha_post_mean))

g1_post <- ggplot(fitPlotData, aes(x=depth, y=count, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("MVGP vs. depth") + 
  geom_line(aes(x=depth, y=alpha, col = species), fitPlotData, lwd=1.25)

g2_post <- ggplot(fitPlotData, aes(x=depth, y=count, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("MVGP vs. depth by species") + 
  geom_line(aes(x=depth, y=alpha, col = species), fitPlotData, lwd=1.25) + 
  facet_wrap( ~ species, ncol = 4)
multiplot(g1_post, g2_post, gsim1, gsim2, cols=2)
```

```{r}
pred <- predictRcppMVGP(dat$y_pred, mu_X=mean(dat$X), s2_X=var(dat$X), 
                           min_X=min(dat$X), max_X=max(dat$X),
                           params=params, samples=samples, 
                           file_name="DM-predict.txt")
```





```{r}
## sorted to increase
idx <- order(dat$X_pred)
n_samples <- length(pred$X[, 1])
X_ci <- apply(pred$X[, idx], 2, sort)[(0.025*n_samples+1):(0.975*n_samples), ]
sim.df <- data.frame(Covariate=c(X_ci),
                     Observation=factor(rep((1:length(dat$X_pred)),
                                            each=n_samples*0.95)),
                     truth=rep(dat$X_pred[idx],
                               each=n_samples*0.95))

ggplot(sim.df, aes(Observation, Covariate)) +
  geom_violin(position="identity") +
  geom_point(aes(Observation, truth), color="red") +
  scale_x_discrete(breaks=seq(5, 25, 5)) + 
  labs(x="Observation", y="Unobserved July Temperature")
```






```{r}
X_train <- dat$X
X_test <- dat$X_pred
y_train <- dat$y
y_train_prop <- y_train
y_test <- dat$y_pred
y_test_prop <- y_test
for (i in 1:N) {
  y_train_prop[i, ] <- y_train_prop[i, ] / sum(y_train_prop[i, ])
}
for (i in 1:N_pred) {
  y_test_prop[i, ] <- y_test_prop[i, ] / sum(y_test_prop[i, ])
}
colnames(y_train_prop) <- letters[1:d]
colnames(y_test_prop) <- letters[1:d]

##
## evaluate predictive ability
##

Rcpp::sourceCpp("~/testate/functions/makeCRPS.cpp")
CRPS_GAM <- makeCRPS(pred$X, X_test, n_samples)
X_mean <- apply(pred$X, 2, mean)
MSPE_GAM <- (X_mean - X_test)^2
MAE_GAM <- abs(apply(pred$X, 2, median) -  X_test)
X_025 <- apply(pred$X, 2, quantile, prob = 0.025)
X_975 <- apply(pred$X, 2, quantile, prob = 0.975)
coverage_GAM <- (X_test >= X_025) & (X_test <= X_975)

## WA reconstruction - subset to deal with all zero occurrence species
zeros_idx <- which(colSums(y_train_prop) == 0)
if (length(zeros_idx) > 0) {
  modWA <- rioja::WA(y_train_prop[, - zeros_idx], X_train)
  predWA <- predict(modWA, y_test_prop[, - zeros_idx], sse=TRUE, nboot=1000)
} else {
  ## no data to subset
  modWA <- rioja::WA(y_train_prop, X_train)
  predWA <- predict(modWA, y_test_prop, sse=TRUE, nboot=1000)      
}
source("~/testate/functions/makeCRPSGauss.R")
CRPS_WA <- makeCRPSGauss(predWA$fit[, 1], sqrt(predWA$v1.boot[, 1]), X_test)
MAE_WA <- abs(predWA$fit[, 1] - X_test)
MSPE_WA <- (predWA$fit[, 1] - X_test)^2
coverage_WA <- (X_test >= 
                  (predWA$fit[, 1] - 2*sqrt(predWA$v1.boot[, 1]))) & 
  (X_test <= (predWA$fit[, 1] + 2*sqrt(predWA$v1.boot[, 1])))


## MLRC reconstruction - subset to deal with all zero occurrence species
zeros_idx <- which(colSums(y_train_prop) == 0)
if (length(zeros_idx) > 0) {
  modMLRC <- rioja::MLRC(y_train_prop[, - zeros_idx], X_train)
  predMLRC <- predict(modMLRC, y_test_prop[, - zeros_idx],
                      sse=TRUE, nboot=1000)
} else {
  modMLRC <- rioja::MLRC(y_train_prop, X_train)
  predMLRC <- predict(modMLRC, y_test_prop, sse=TRUE, nboot=1000)
}
source("~/testate/functions/makeCRPSGauss.R")
CRPS_MLRC <- makeCRPSGauss(predMLRC$fit[, 1], sqrt(predMLRC$v1.boot[, 1]),
                      X_test)
MSPE_MLRC <- (predMLRC$fit[, 1] - X_test)^2
MAE_MLRC <- abs(predMLRC$fit[, 1] - X_test)
coverage_MLRC <- ( X_test >= (predMLRC$fit[, 1] - 2*sqrt(predMLRC$v1.boot[, 1]))) & 
  (X_test <= (predMLRC$fit[, 1] + 2 * sqrt(predMLRC$v1.boot[, 1])))

## Modern analogue technique
modMAT <- mat(y_train, X_train)
predMAT <- predict(modMAT, y_test, bootstrap=TRUE, n.boot=1000)
CRPS_MAT <- makeCRPSGauss(
  apply(predMAT$predictions$model$predicted, 2, mean), 
  apply(predMAT$predictions$model$predicted, 2, sd), X_test)
MSPE_MAT <- (apply(predMAT$predictions$model$predicted, 2, mean) - X_test)^2
MAE_MAT <- abs(  apply(predMAT$predictions$model$predicted, 2, median) - X_test)
coverage_MAT <- ( X_test >= (apply(predMAT$predictions$model$predicted, 2, mean) -
                                2*  apply(predMAT$predictions$model$predicted, 2, sd)) & 
  (X_test <= (apply(predMAT$predictions$model$predicted, 2, mean) +
                                2*  apply(predMAT$predictions$model$predicted, 2, sd))))


## GJAM model fit
idx_hold <- (N+1):(N+N_pred)
Xdf <- data.frame(x=c(X_train, X_test))
Xdf$x[idx_hold] <- NA
ydf <- data.frame(as.matrix(rbind(y_train, y_test)))
colnames(ydf) <- paste("y", 1:dim(y_train)[2], sep="")
ml <- list(ng = 5000, burnin = 500, typeNames = rep("CC", dim(y_train)[2]), 
           PREDICTX=TRUE)
## fit second order polynomial model
out <- gjam(~ x + I(x^2), Xdf, ydf, ml)
xMu  <- out$prediction$xpredMu[idx_hold, 2]        #inverse prediction of x
xSd  <- out$prediction$xpredSd[idx_hold, 2]  

##
source("~/testate/functions/makeCRPSGauss.R")
CRPS_GJAM <- makeCRPSGauss(xMu, xSd, X_test)
MSPE_GJAM <- (xMu - X_test)^2
MAE_GJAM <- abs(xMu - X_test)
X_025 <- xMu - 2*xSd
X_975 <- xMu + 2*xSd
coverage_GJAM <- (X_test >= X_025) & (X_test <= X_975)

```



```{r}
CRPS_out <- cbind(CRPS_GAM, CRPS_WA, CRPS_MAT, CRPS_MLRC, CRPS_GJAM)
MSPE_out <- cbind(MSPE_GAM, MSPE_WA, MSPE_MAT, MSPE_MLRC, MSPE_GJAM)
MAE_out <- cbind(MAE_GAM, MAE_WA, MAE_MAT, MAE_MLRC, MAE_GJAM)
coverage_out <- cbind(coverage_GAM, coverage_WA, coverage_MAT, coverage_MLRC,
                      coverage_GJAM)
colnames(CRPS_out) <- c("GAM", "WA", "MAT", "MLRC", "GJAM")
colnames(MAE_out) <- c("GAM", "WA", "MAT", "MLRC", "GJAM")
colnames(MSPE_out) <- c("GAM", "WA", "MAT", "MLRC", "GJAM")
colnames(coverage_out) <- c("GAM", "WA", "MAT", "MLRC", "GJAM")

CRPS <- data.frame(t(apply(CRPS_out, 2, mean)))
MSPE <- data.frame(t(apply(MSPE_out, 2, mean)))
MAE <- data.frame(t(apply(MAE_out, 2, mean)))
coverage <- data.frame(100/(N_pred)*t(apply(coverage_out, 2, sum)))

sim_results <- rbind(CRPS, MSPE, MAE, coverage)
rownames(sim_results) <- c("CRPS", "MSPE", "MAE", "95% CI coverage rates")
# print(xtable(sim_results, digits=4), file="results/sim-dm.tex",
#       floating=FALSE)
```

```{r}
kable(sim_results)
```
